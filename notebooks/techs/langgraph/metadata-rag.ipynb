{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Metadata Filtering in RAG Workflow\n",
    "\n",
    "This notebook demonstrates how to implement and test metadata filtering in a Retrieval-Augmented Generation (RAG) workflow based on the Multi-Meta-RAG approach. This approach significantly improves results on multi-hop queries by using LLM-extracted metadata for database filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, let's install the necessary dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install langchain langchain-community langchain-anthropic sentence-transformers retry\n",
    "!pip install openai neo4j python-dotenv chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import ast\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set up OpenAI API (for metadata extraction)\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sample Document Corpus\n",
    "\n",
    "Let's create a small sample corpus with metadata to test our approach. In a real application, you might load documents from files or a database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small sample corpus with metadata\n",
    "sample_corpus = [\n",
    "    {\n",
    "        \"title\": \"MacBook Air Discount\",\n",
    "        \"content\": \"Apple's 13.6-inch MacBook Air with the M2 chip is currently discounted by $150, bringing the price down to $1,049. This is one of the best deals we've seen for this model in recent months.\",\n",
    "        \"source\": \"Engadget\",\n",
    "        \"published_at\": \"March 10, 2025\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Galaxy Buds Sale\",\n",
    "        \"content\": \"Samsung's Galaxy Buds 2 are now on sale for just $99, a $50 discount from their regular price. These wireless earbuds offer excellent noise cancellation and sound quality.\",\n",
    "        \"source\": \"The Verge\",\n",
    "        \"published_at\": \"March 15, 2025\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Tech Discounts Roundup\",\n",
    "        \"content\": \"This week's best tech deals include discounts on Apple MacBooks, Samsung earbuds, and various smart home devices. Retailers are offering these discounts ahead of upcoming product refreshes.\",\n",
    "        \"source\": \"TechCrunch\",\n",
    "        \"published_at\": \"March 12, 2025\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"New Pixel Phone Launch\",\n",
    "        \"content\": \"Google has announced the launch date for its next Pixel phone. The device is expected to feature significant camera improvements and a new custom processor.\",\n",
    "        \"source\": \"Wired\",\n",
    "        \"published_at\": \"March 5, 2025\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Streaming Service Price Increase\",\n",
    "        \"content\": \"Netflix has announced a price increase for all subscription tiers starting next month. The basic plan will increase by $2, while premium subscriptions will see a $3 increase.\",\n",
    "        \"source\": \"The New York Times\",\n",
    "        \"published_at\": \"March 8, 2025\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Convert the sample corpus to LangChain documents\n",
    "documents = []\n",
    "for item in sample_corpus:\n",
    "    doc = Document(\n",
    "        page_content=f\"{item['title']}\\n\\n{item['content']}\",\n",
    "        metadata={\n",
    "            \"title\": item[\"title\"],\n",
    "            \"source\": item[\"source\"],\n",
    "            \"published_at\": item[\"published_at\"],\n",
    "        }\n",
    "    )\n",
    "    documents.append(doc)\n",
    "\n",
    "# Display the documents\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"Document {i+1}:\")\n",
    "    print(f\"Content: {doc.page_content[:100]}...\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chunk Documents and Create Vector Store\n",
    "\n",
    "Now we'll split the documents into chunks and create a vector store with the documents and their metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=256,\n",
    "    chunk_overlap=32,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "# Split the documents and preserve metadata\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Created {len(chunks)} chunks from {len(documents)} documents.\")\n",
    "print(f\"\\nSample chunk: {chunks[0].page_content}\")\n",
    "print(f\"Sample chunk metadata: {chunks[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embedding model\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\",  # Using a smaller model for demo purposes\n",
    "    model_kwargs={\"device\": \"cpu\"}\n",
    ")\n",
    "\n",
    "# Create a vector store with the chunks\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./chroma_db\",\n",
    "    collection_metadata={\"hnsw:space\": \"cosine\"}\n",
    ")\n",
    "\n",
    "print(\"Vector store created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Metadata Extraction Using LLM\n",
    "\n",
    "Now we'll implement the metadata extraction function using an LLM (in this case, OpenAI's GPT model). This function will extract relevant metadata from queries to use for filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template for metadata extraction\n",
    "EXTRACT_FILTER_TEMPLATE = \"\"\"Given the question, extract the metadata to filter the database about article sources and dates.\n",
    "The sources can only be from the list: ['Engadget', 'The Verge', 'TechCrunch', 'Wired', 'The New York Times']\n",
    "\n",
    "Examples to follow:\n",
    "\n",
    "Question: Who is the individual associated with the cryptocurrency industry facing a criminal trial on fraud and conspiracy charges, as reported by both The Verge and TechCrunch?\n",
    "Answer: {'source': {'$in': ['The Verge', 'TechCrunch']}}\n",
    "\n",
    "Question: Did Engadget report a discount on the MacBook Air before TechCrunch published an article about tech discounts?\n",
    "Answer: {'source': {'$in': ['Engadget', 'TechCrunch']}}\n",
    "\n",
    "Question: What did The New York Times report about subscription price increases in March 2025?\n",
    "Answer: {'source': {'$in': ['The New York Times']}, 'published_at': {'$in': ['March 2025']}}\n",
    "\n",
    "Now it is your turn:\n",
    "\n",
    "Question: {query}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "def extract_metadata_filter(query: str) -> Dict[str, Any]:\n",
    "    \"\"\"Extract metadata filter from a query using OpenAI's GPT model.\"\"\"\n",
    "    try:\n",
    "        # Call the OpenAI API\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": EXTRACT_FILTER_TEMPLATE.format(query=query)}],\n",
    "            temperature=0.1,\n",
    "        )\n",
    "        \n",
    "        # Extract the filter from the response\n",
    "        filter_str = completion.choices[0].message.content\n",
    "        \n",
    "        # Convert string to dictionary\n",
    "        filter_dict = ast.literal_eval(filter_str)\n",
    "        \n",
    "        print(f\"Extracted filter: {filter_dict}\")\n",
    "        return filter_dict\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting metadata filter: {e}\")\n",
    "        # Return empty filter if extraction fails\n",
    "        return {}\n",
    "\n",
    "# Test the metadata extraction function with a sample query\n",
    "sample_query = \"Did Engadget report a discount on the MacBook Air before The Verge published an article about Samsung Galaxy Buds?\"\n",
    "metadata_filter = extract_metadata_filter(sample_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implement Retrieval with Metadata Filtering\n",
    "\n",
    "Now let's implement the retrieval function that uses both semantic search and metadata filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_with_metadata_filter(query: str, metadata_filter: Dict[str, Any], k: int = 3) -> List[Document]:\n",
    "    \"\"\"Retrieve documents using both semantic search and metadata filtering.\"\"\"\n",
    "    try:\n",
    "        # Convert the filter to the format expected by Chroma\n",
    "        chroma_filter = {}\n",
    "        \n",
    "        if \"source\" in metadata_filter:\n",
    "            if \"$in\" in metadata_filter[\"source\"]:\n",
    "                chroma_filter[\"source\"] = {\"$in\": metadata_filter[\"source\"][\"$in\"]}\n",
    "        \n",
    "        if \"published_at\" in metadata_filter:\n",
    "            if \"$in\" in metadata_filter[\"published_at\"]:\n",
    "                chroma_filter[\"published_at\"] = {\"$in\": metadata_filter[\"published_at\"][\"$in\"]}\n",
    "        \n",
    "        print(f\"Chroma filter: {chroma_filter}\")\n",
    "        \n",
    "        # Retrieve documents with filter\n",
    "        docs = vectordb.similarity_search(\n",
    "            query=query,\n",
    "            k=k,\n",
    "            filter=chroma_filter if chroma_filter else None\n",
    "        )\n",
    "        \n",
    "        return docs\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving documents: {e}\")\n",
    "        return []\n",
    "\n",
    "# Test retrieval with metadata filtering\n",
    "filtered_docs = retrieve_with_metadata_filter(sample_query, metadata_filter)\n",
    "\n",
    "print(f\"\\nRetrieved {len(filtered_docs)} documents with metadata filtering:\")\n",
    "for i, doc in enumerate(filtered_docs):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print(f\"Metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare with Regular Retrieval (Without Metadata Filtering)\n",
    "\n",
    "Let's compare the results with and without metadata filtering to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve documents without metadata filtering\n",
    "regular_docs = vectordb.similarity_search(query=sample_query, k=3)\n",
    "\n",
    "print(f\"Retrieved {len(regular_docs)} documents without metadata filtering:\")\n",
    "for i, doc in enumerate(regular_docs):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "\n",
    "# Compare the results\n",
    "print(\"\\n--- Comparison ---\")\n",
    "print(\"Documents retrieved with metadata filtering:\")\n",
    "filtered_sources = [doc.metadata[\"source\"] for doc in filtered_docs]\n",
    "print(filtered_sources)\n",
    "\n",
    "print(\"\\nDocuments retrieved without metadata filtering:\")\n",
    "regular_sources = [doc.metadata[\"source\"] for doc in regular_docs]\n",
    "print(regular_sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. End-to-End RAG with Metadata Filtering\n",
    "\n",
    "Now let's implement a complete RAG pipeline that uses metadata filtering and compare it with a standard RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM\n",
    "llm = ChatAnthropic(model=\"claude-3-sonnet-20240229\", temperature=0)\n",
    "\n",
    "# Define the RAG prompt\n",
    "rag_prompt_template = \"\"\"Answer the following question based on the provided context. If the context doesn't contain the necessary information, state that you don't know.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = PromptTemplate(\n",
    "    template=rag_prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rag(query: str, use_metadata_filtering: bool = False) -> str:\n",
    "    \"\"\"Run the RAG pipeline with or without metadata filtering.\"\"\"\n",
    "    # Step 1: Retrieve relevant documents\n",
    "    if use_metadata_filtering:\n",
    "        # Extract metadata filter\n",
    "        metadata_filter = extract_metadata_filter(query)\n",
    "        # Retrieve with metadata filtering\n",
    "        retrieved_docs = retrieve_with_metadata_filter(query, metadata_filter)\n",
    "    else:\n",
    "        # Regular retrieval without filtering\n",
    "        retrieved_docs = vectordb.similarity_search(query=query, k=3)\n",
    "    \n",
    "    # Step 2: Prepare the context\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "    \n",
    "    # Step 3: Generate the answer using the LLM\n",
    "    chain = LLMChain(llm=llm, prompt=rag_prompt)\n",
    "    response = chain.run(context=context, question=query)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries for comparison\n",
    "test_query = \"Did Engadget report a discount on the MacBook Air before The Verge published an article about Samsung Galaxy Buds?\"\n",
    "\n",
    "# Run the regular RAG pipeline\n",
    "print(\"Running regular RAG (without metadata filtering)...\")\n",
    "regular_rag_response = run_rag(test_query, use_metadata_filtering=False)\n",
    "\n",
    "# Run the RAG pipeline with metadata filtering\n",
    "print(\"\\nRunning RAG with metadata filtering...\")\n",
    "filtered_rag_response = run_rag(test_query, use_metadata_filtering=True)\n",
    "\n",
    "# Compare the responses\n",
    "print(\"\\n--- Regular RAG Response ---\")\n",
    "print(regular_rag_response)\n",
    "\n",
    "print(\"\\n--- Metadata-Filtered RAG Response ---\")\n",
    "print(filtered_rag_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Experiment with Different Queries\n",
    "\n",
    "Let's test a few more queries to see how metadata filtering helps with different types of questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries = [\n",
    "    \"What did Engadget and The Verge report about tech discounts recently?\",\n",
    "    \"Did The New York Times discuss subscription price increases in March 2025?\",\n",
    "    \"What products were on sale according to TechCrunch's discount roundup?\"\n",
    "]\n",
    "\n",
    "for i, query in enumerate(test_queries):\n",
    "    print(f\"\\n--- Query {i+1}: {query} ---\")\n",
    "    \n",
    "    # With metadata filtering\n",
    "    print(\"\\nMetadata-Filtered RAG Response:\")\n",
    "    filtered_response = run_rag(query, use_metadata_filtering=True)\n",
    "    print(filtered_response)\n",
    "    \n",
    "    # Without metadata filtering\n",
    "    print(\"\\nRegular RAG Response:\")\n",
    "    regular_response = run_rag(query, use_metadata_filtering=False)\n",
    "    print(regular_response)\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to implement and test metadata filtering in a RAG workflow. The key findings are:\n",
    "\n",
    "1. Metadata filtering helps ensure that only documents from relevant sources are retrieved for multi-hop queries\n",
    "2. LLM-based metadata extraction can automatically identify the relevant sources and dates from the query\n",
    "3. The combination of semantic search and metadata filtering provides more accurate results than semantic search alone\n",
    "4. This approach is particularly useful for questions that reference specific sources or time periods\n",
    "\n",
    "This implementation is a simplified version of the Multi-Meta-RAG approach described in the paper. For production use, you might want to consider using a more sophisticated vector database like Neo4j (as used in the original implementation) and optimizing the metadata extraction prompt for your specific use case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Langgraph Python 3.11",
   "language": "python",
   "name": "langgraph-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
