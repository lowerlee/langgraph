{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Government Documents RAG System with LangGraph\n",
    "\n",
    "This notebook demonstrates how to build a RAG (Retrieval-Augmented Generation) system for government documents using LangGraph and Claude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "# import fitz  # PyMuPDF\n",
    "from typing import Annotated, List, Dict, Any, TypedDict, Optional\n",
    "from typing_extensions import TypedDict\n",
    "import sys\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader, TextLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_chroma import Chroma as LCChroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.schema import Document\n",
    "\n",
    "# LangGraph imports\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "set_env(\"ANTHROPIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Document Processing\n",
    "\n",
    "We'll create a class to handle loading and processing documents from a directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "from typing import List, Optional, Dict, Any\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.schema import Document\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self, docs_dir: str, chunk_size: int = 1000, chunk_overlap: int = 200):\n",
    "        self.docs_dir = docs_dir\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=self.chunk_size,\n",
    "            chunk_overlap=self.chunk_overlap,\n",
    "            length_function=len,\n",
    "        )\n",
    "        \n",
    "    def extract_text_with_layout(self, pdf_path: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Extract text from PDF while preserving layout information.\n",
    "        Uses PyMuPDF (fitz) to get better results with complex layouts.\n",
    "        \"\"\"\n",
    "        doc = fitz.open(pdf_path)\n",
    "        extracted_data = []\n",
    "        \n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc.load_page(page_num)\n",
    "            \n",
    "            # Get page dimensions to determine orientation\n",
    "            width, height = page.rect.width, page.rect.height\n",
    "            orientation = \"landscape\" if width > height else \"portrait\"\n",
    "            \n",
    "            # Extract text blocks with their bounding boxes\n",
    "            blocks = page.get_text(\"blocks\")\n",
    "            \n",
    "            # Sort blocks - for dual column papers, sort by x coordinate first then y\n",
    "            if self._is_likely_multi_column(blocks):\n",
    "                # Use x coordinate to identify columns, then sort by y within columns\n",
    "                col_threshold = width / 2  # Simple assumption of a two-column layout\n",
    "                left_col = [b for b in blocks if b[0] < col_threshold]\n",
    "                right_col = [b for b in blocks if b[0] >= col_threshold]\n",
    "                \n",
    "                # Sort each column by y-coordinate\n",
    "                left_col.sort(key=lambda b: b[1])\n",
    "                right_col.sort(key=lambda b: b[1])\n",
    "                \n",
    "                # Process left column, then right column\n",
    "                sorted_blocks = left_col + right_col\n",
    "            else:\n",
    "                # Sort blocks by y-coordinate for standard top-to-bottom reading\n",
    "                sorted_blocks = sorted(blocks, key=lambda b: b[1])\n",
    "            \n",
    "            for block in sorted_blocks:\n",
    "                # Each block is (x0, y0, x1, y1, text, block_type, block_no)\n",
    "                block_text = block[4]\n",
    "                if block_text.strip():  # Skip empty blocks\n",
    "                    extracted_data.append({\n",
    "                        \"page\": page_num + 1,\n",
    "                        \"text\": block_text,\n",
    "                        \"orientation\": orientation,\n",
    "                        \"bbox\": block[:4],  # Bounding box coordinates\n",
    "                    })\n",
    "        \n",
    "        return extracted_data\n",
    "    \n",
    "    def _is_likely_multi_column(self, blocks, threshold=0.3):\n",
    "        \"\"\"\n",
    "        Detect if a page likely has a multi-column layout.\n",
    "        \"\"\"\n",
    "        if not blocks:\n",
    "            return False\n",
    "            \n",
    "        # Extract x-coordinates where blocks start\n",
    "        x_starts = [b[0] for b in blocks]\n",
    "        \n",
    "        # Get page width from the maximum x-coordinate of any block\n",
    "        page_width = max(b[2] for b in blocks)\n",
    "        \n",
    "        # Count blocks that start in left vs right half of page\n",
    "        left_half = sum(1 for x in x_starts if x < page_width/2)\n",
    "        right_half = sum(1 for x in x_starts if x >= page_width/2)\n",
    "        \n",
    "        # If there's a significant number of blocks in both halves, likely multi-column\n",
    "        total_blocks = len(blocks)\n",
    "        return (left_half / total_blocks > threshold and \n",
    "                right_half / total_blocks > threshold)\n",
    "    \n",
    "    def load_documents(self) -> List[Document]:\n",
    "        \"\"\"Load documents from the specified directory with enhanced PDF processing\"\"\"\n",
    "        # Create directory if it doesn't exist\n",
    "        if not os.path.exists(self.docs_dir):\n",
    "            os.makedirs(self.docs_dir)\n",
    "            print(f\"Created directory: {self.docs_dir}\")\n",
    "            return []\n",
    "        \n",
    "        # Process all PDF files with enhanced layout handling\n",
    "        pdf_docs = []\n",
    "        pdf_files = [f for f in os.listdir(self.docs_dir) \n",
    "                     if f.lower().endswith('.pdf')]\n",
    "        \n",
    "        for filename in pdf_files:\n",
    "            filepath = os.path.join(self.docs_dir, filename)\n",
    "            try:\n",
    "                print(f\"Processing PDF: {filename}\")\n",
    "                extracted_data = self.extract_text_with_layout(filepath)\n",
    "                \n",
    "                # Group content by page\n",
    "                pages = {}\n",
    "                for item in extracted_data:\n",
    "                    page_num = item[\"page\"]\n",
    "                    if page_num not in pages:\n",
    "                        pages[page_num] = []\n",
    "                    pages[page_num].append(item)\n",
    "                \n",
    "                # Create a document for each page\n",
    "                for page_num, page_items in pages.items():\n",
    "                    page_text = \"\\n\".join([item[\"text\"] for item in page_items])\n",
    "                    \n",
    "                    # Clean up text - replace excessive newlines and spaces\n",
    "                    page_text = re.sub(r'\\n{3,}', '\\n\\n', page_text)\n",
    "                    page_text = re.sub(r' {2,}', ' ', page_text)\n",
    "                    \n",
    "                    # Create document with metadata\n",
    "                    doc = Document(\n",
    "                        page_content=page_text,\n",
    "                        metadata={\n",
    "                            \"source\": filepath,\n",
    "                            \"page\": page_num,\n",
    "                            \"file_name\": filename,\n",
    "                        }\n",
    "                    )\n",
    "                    pdf_docs.append(doc)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {str(e)}\")\n",
    "        \n",
    "        # Load text files with standard TextLoader\n",
    "        text_docs = []\n",
    "        text_files = [f for f in os.listdir(self.docs_dir) \n",
    "                     if f.lower().endswith('.txt')]\n",
    "        \n",
    "        if text_files:\n",
    "            text_loader = DirectoryLoader(self.docs_dir, glob=\"**/*.txt\", loader_cls=TextLoader)\n",
    "            text_docs = text_loader.load()\n",
    "        \n",
    "        # Combine all documents\n",
    "        all_docs = pdf_docs + text_docs\n",
    "        \n",
    "        print(f\"Loaded {len(all_docs)} documents\")\n",
    "        return all_docs\n",
    "    \n",
    "    def process_documents(self, documents: List[Document]) -> List[Document]:\n",
    "        \"\"\"Split documents into chunks for vectorization\"\"\"\n",
    "        if not documents:\n",
    "            print(\"No documents to process\")\n",
    "            return []\n",
    "            \n",
    "        chunked_documents = self.text_splitter.split_documents(documents)\n",
    "        print(f\"Split into {len(chunked_documents)} chunks\")\n",
    "        return chunked_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "libGL.so.1: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mschema\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Import unstructured components\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpartition\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpdf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m partition_pdf\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpartition\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m partition_text\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocuments\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01melements\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Element, Text, Title\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/unstructured/partition/pdf.py:19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image \u001b[38;5;28;01mas\u001b[39;00m PILImage\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpypdf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PdfReader\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured_inference\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minference\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayout\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DocumentLayout\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured_inference\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minference\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayoutelement\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LayoutElement\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchunking\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m add_chunking_strategy\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/unstructured_inference/inference/layout.py:18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured_inference\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minference\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayoutelement\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LayoutElement, LayoutElements\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured_inference\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlogger\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logger\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured_inference\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_model\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured_inference\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01munstructuredmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     20\u001b[39m     UnstructuredElementExtractionModel,\n\u001b[32m     21\u001b[39m     UnstructuredObjectDetectionModel,\n\u001b[32m     22\u001b[39m )\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured_inference\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvisualize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m draw_bbox\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/unstructured_inference/models/base.py:7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, Optional, Tuple, Type\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured_inference\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdetectron2onnx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      8\u001b[39m     MODEL_TYPES \u001b[38;5;28;01mas\u001b[39;00m DETECTRON2_ONNX_MODEL_TYPES,\n\u001b[32m      9\u001b[39m )\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured_inference\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdetectron2onnx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m UnstructuredDetectronONNXModel\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured_inference\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01munstructuredmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m UnstructuredModel\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/unstructured_inference/models/detectron2onnx.py:4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, Final, List, Optional, Union, cast\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01monnxruntime\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: libGL.so.1: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List, Optional, Dict, Any\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Import unstructured components\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.partition.text import partition_text\n",
    "from unstructured.documents.elements import Element, Text, Title\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self, docs_dir: str, chunk_size: int = 1000, chunk_overlap: int = 200):\n",
    "        self.docs_dir = docs_dir\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=self.chunk_size,\n",
    "            chunk_overlap=self.chunk_overlap,\n",
    "            length_function=len,\n",
    "        )\n",
    "        \n",
    "    def process_pdf_with_unstructured(self, pdf_path: str) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Process PDF using unstructured library with high resolution strategy\n",
    "        for better handling of complex layouts.\n",
    "        \"\"\"\n",
    "        filename = os.path.basename(pdf_path)\n",
    "        \n",
    "        try:\n",
    "            # Use hi_res strategy for better layout detection\n",
    "            elements = partition_pdf(\n",
    "                pdf_path, \n",
    "                strategy=\"hi_res\",  # Better for complex layouts like academic papers\n",
    "                infer_table_structure=True,  # Better handling of tables\n",
    "                extract_images_in_pdf=False,  # Skip image extraction to speed up processing\n",
    "                max_partition=None  # Process all pages\n",
    "            )\n",
    "            \n",
    "            # Group elements by page\n",
    "            pages = {}\n",
    "            for element in elements:\n",
    "                page_num = getattr(element, \"metadata\", {}).get(\"page_number\", 0)\n",
    "                if page_num not in pages:\n",
    "                    pages[page_num] = []\n",
    "                pages[page_num].append(element)\n",
    "            \n",
    "            # Create documents by page\n",
    "            documents = []\n",
    "            for page_num, page_elements in pages.items():\n",
    "                # Extract text from elements\n",
    "                text_elements = []\n",
    "                for element in page_elements:\n",
    "                    # Skip non-text elements\n",
    "                    if hasattr(element, \"text\"):\n",
    "                        # Format based on element type\n",
    "                        if isinstance(element, Title):\n",
    "                            text_elements.append(f\"## {element.text}\")\n",
    "                        else:\n",
    "                            text_elements.append(element.text)\n",
    "                \n",
    "                # Join text with appropriate spacing\n",
    "                page_text = \"\\n\\n\".join(text_elements)\n",
    "                \n",
    "                # Create document with metadata\n",
    "                doc = Document(\n",
    "                    page_content=page_text,\n",
    "                    metadata={\n",
    "                        \"source\": pdf_path,\n",
    "                        \"page\": page_num,\n",
    "                        \"file_name\": filename,\n",
    "                    }\n",
    "                )\n",
    "                documents.append(doc)\n",
    "            \n",
    "            return documents\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def load_documents(self) -> List[Document]:\n",
    "        \"\"\"Load documents from the specified directory with enhanced processing\"\"\"\n",
    "        # Create directory if it doesn't exist\n",
    "        if not os.path.exists(self.docs_dir):\n",
    "            os.makedirs(self.docs_dir)\n",
    "            print(f\"Created directory: {self.docs_dir}\")\n",
    "            return []\n",
    "        \n",
    "        # Process all PDF files with unstructured\n",
    "        all_docs = []\n",
    "        pdf_files = [f for f in os.listdir(self.docs_dir) \n",
    "                     if f.lower().endswith('.pdf')]\n",
    "        \n",
    "        for filename in pdf_files:\n",
    "            filepath = os.path.join(self.docs_dir, filename)\n",
    "            print(f\"Processing PDF: {filename}\")\n",
    "            pdf_docs = self.process_pdf_with_unstructured(filepath)\n",
    "            all_docs.extend(pdf_docs)\n",
    "            \n",
    "        # Process text files with unstructured or standard loader\n",
    "        text_files = [f for f in os.listdir(self.docs_dir) \n",
    "                     if f.lower().endswith('.txt')]\n",
    "        \n",
    "        if text_files:\n",
    "            # Option 1: Use standard loader (faster, less sophisticated)\n",
    "            text_loader = DirectoryLoader(self.docs_dir, glob=\"**/*.txt\", loader_cls=TextLoader)\n",
    "            text_docs = text_loader.load()\n",
    "            all_docs.extend(text_docs)\n",
    "            \n",
    "            # Option 2: Use unstructured for text files too (uncomment if needed)\n",
    "            # for filename in text_files:\n",
    "            #     filepath = os.path.join(self.docs_dir, filename)\n",
    "            #     with open(filepath, \"r\") as f:\n",
    "            #         text = f.read()\n",
    "            #     elements = partition_text(text)\n",
    "            #     text_content = \"\\n\\n\".join([e.text for e in elements if hasattr(e, \"text\")])\n",
    "            #     doc = Document(\n",
    "            #         page_content=text_content,\n",
    "            #         metadata={\"source\": filepath, \"file_name\": filename}\n",
    "            #     )\n",
    "            #     all_docs.append(doc)\n",
    "        \n",
    "        print(f\"Loaded {len(all_docs)} documents\")\n",
    "        return all_docs\n",
    "    \n",
    "    def process_documents(self, documents: List[Document]) -> List[Document]:\n",
    "        \"\"\"Split documents into chunks for vectorization\"\"\"\n",
    "        if not documents:\n",
    "            print(\"No documents to process\")\n",
    "            return []\n",
    "            \n",
    "        chunked_documents = self.text_splitter.split_documents(documents)\n",
    "        print(f\"Split into {len(chunked_documents)} chunks\")\n",
    "        return chunked_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Vector Store Setup\n",
    "\n",
    "Now let's create a vector store for document embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    def __init__(self, persist_directory: str = \"./chroma_db\"):\n",
    "        self.persist_directory = persist_directory\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"all-MiniLM-L6-v2\",\n",
    "            model_kwargs={'device': 'cpu'},\n",
    "        )\n",
    "        self.vectordb = None\n",
    "        \n",
    "    def create_or_load_db(self, documents: Optional[List[Document]] = None) -> Chroma:\n",
    "        \"\"\"Create a new vector store or load existing one\"\"\"\n",
    "        # Create directory if it doesn't exist\n",
    "        if not os.path.exists(self.persist_directory):\n",
    "            os.makedirs(self.persist_directory)\n",
    "            \n",
    "        if documents and len(documents) > 0:\n",
    "            # Create a new vector store\n",
    "            print(f\"Creating new vector database with {len(documents)} documents\")\n",
    "            self.vectordb = LCChroma.from_documents(\n",
    "                documents=documents,\n",
    "                embedding=self.embeddings,\n",
    "                persist_directory=self.persist_directory,\n",
    "            )\n",
    "            # The persist() call is no longer needed - removal fixes the error\n",
    "            # LCChroma automatically persists when persist_directory is provided\n",
    "            print(f\"Vector database created and persisted to {self.persist_directory}\")\n",
    "        else:\n",
    "            # Load existing vector store if it exists\n",
    "            try:\n",
    "                print(f\"Loading existing vector database from {self.persist_directory}\")\n",
    "                self.vectordb = LCChroma(\n",
    "                    persist_directory=self.persist_directory,\n",
    "                    embedding_function=self.embeddings,\n",
    "                )\n",
    "                print(f\"Successfully loaded vector database with {self.vectordb._collection.count()} documents\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading existing database: {e}\")\n",
    "                print(\"Creating an empty vector database instead\")\n",
    "                self.vectordb = LCChroma(\n",
    "                    embedding_function=self.embeddings,\n",
    "                    persist_directory=self.persist_directory,\n",
    "                )\n",
    "        \n",
    "        return self.vectordb\n",
    "    \n",
    "    def get_retriever(self, k: int = 4):\n",
    "        \"\"\"Get a retriever from the vector store\"\"\"\n",
    "        if self.vectordb is None:\n",
    "            raise ValueError(\"VectorDB not initialized. Call create_or_load_db first.\")\n",
    "        \n",
    "        return self.vectordb.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": k}\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build the RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    retrieved_documents: Optional[List[Document]]\n",
    "\n",
    "class GovernmentDocsRAG:\n",
    "    def __init__(self, model_name: str = \"claude-3-5-sonnet-20240620\"):\n",
    "        self.memory = MemorySaver()\n",
    "        self.llm = ChatAnthropic(model=model_name)\n",
    "        \n",
    "        # Define system prompt for the chatbot\n",
    "        self.system_prompt = \"\"\"You are a helpful assistant specialized in answering questions about government documents. \n",
    "When responding:\n",
    "1. Base your answers on the retrieved documents\n",
    "2. If the documents don't contain the answer, say you don't know\n",
    "3. Always cite your sources by referencing the document name and section\n",
    "4. Be concise but thorough in your responses\n",
    "5. Format your response in a readable way\n",
    "\n",
    "Current retrieved documents for this query: {documents}\n",
    "\"\"\"\n",
    "        \n",
    "        # Initialize the graph\n",
    "        self.graph_builder = StateGraph(State)\n",
    "        \n",
    "    def build_graph(self, retriever):\n",
    "        \"\"\"Build the LangGraph flow\"\"\"\n",
    "        # Define the retriever node\n",
    "        def retrieve_documents(state: State) -> Dict[str, Any]:\n",
    "            \"\"\"Retrieve relevant documents based on the user query\"\"\"\n",
    "            # Get the latest user message\n",
    "            user_message = state[\"messages\"][-1].content\n",
    "            \n",
    "            # Retrieve documents\n",
    "            retrieved_docs = retriever.get_relevant_documents(user_message)\n",
    "            \n",
    "            # Format documents for better readability\n",
    "            return {\"retrieved_documents\": retrieved_docs}\n",
    "        \n",
    "        # Define the RAG node\n",
    "        def generate_response(state: State) -> Dict[str, Any]:\n",
    "            \"\"\"Generate a response using the LLM with retrieved documents\"\"\"\n",
    "            # Get documents\n",
    "            docs = state.get(\"retrieved_documents\", [])\n",
    "            \n",
    "            # Format documents for the prompt\n",
    "            doc_strings = []\n",
    "            for i, doc in enumerate(docs):\n",
    "                doc_strings.append(f\"Document {i+1}:\\nContent: {doc.page_content}\\nSource: {doc.metadata.get('source', 'Unknown')}\")\n",
    "            \n",
    "            formatted_docs = \"\\n\\n\".join(doc_strings)\n",
    "            \n",
    "            # Create prompt with documents\n",
    "            messages = state[\"messages\"].copy()\n",
    "            \n",
    "            # Add system prompt with documents\n",
    "            system_prompt_with_docs = self.system_prompt.format(documents=formatted_docs)\n",
    "            \n",
    "            # Create augmented LLM\n",
    "            llm_with_docs = self.llm.bind(system=system_prompt_with_docs)\n",
    "            \n",
    "            # Generate response\n",
    "            return {\"messages\": [llm_with_docs.invoke(messages)]}\n",
    "        \n",
    "        # Add nodes to the graph\n",
    "        self.graph_builder.add_node(\"retrieve\", retrieve_documents)\n",
    "        self.graph_builder.add_node(\"rag_response\", generate_response)\n",
    "        \n",
    "        # Add edges\n",
    "        self.graph_builder.add_edge(START, \"retrieve\")\n",
    "        self.graph_builder.add_edge(\"retrieve\", \"rag_response\")\n",
    "        self.graph_builder.add_edge(\"rag_response\", END)\n",
    "        \n",
    "        # Compile the graph\n",
    "        self.graph = self.graph_builder.compile(checkpointer=self.memory)\n",
    "        \n",
    "        return self.graph\n",
    "    \n",
    "    def chat(self, user_input: str, thread_id: str = \"default\"):\n",
    "        \"\"\"Chat with the RAG system\"\"\"\n",
    "        config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "        \n",
    "        result = self.graph.invoke(\n",
    "            {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
    "            config,\n",
    "        )\n",
    "        \n",
    "        # Return the AI's response\n",
    "        return result[\"messages\"][-1].content\n",
    "    \n",
    "    def get_conversation_history(self, thread_id: str = \"default\"):\n",
    "        \"\"\"Get the full conversation history\"\"\"\n",
    "        config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "        state = self.graph.get_state(config)\n",
    "        \n",
    "        # Return all messages if available\n",
    "        if state and \"messages\" in state.values:\n",
    "            return state.values[\"messages\"]\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Set Up the RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the sample documents\n",
    "doc_processor = DocumentProcessor(docs_dir=\"./documents\")\n",
    "raw_docs = doc_processor.load_documents()\n",
    "processed_docs = doc_processor.process_documents(raw_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the vector store\n",
    "vector_store = VectorStore(persist_directory=\"./embeddings_db\")\n",
    "vector_db = vector_store.create_or_load_db(documents=processed_docs)\n",
    "retriever = vector_store.get_retriever(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the RAG system\n",
    "rag_system = GovernmentDocsRAG()\n",
    "rag_system.build_graph(retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Example Interactions\n",
    "\n",
    "Let's interact with our RAG system by asking questions about the government documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Ask about education policy\n",
    "question1 = \"How many titles are there in division B, and what is division B about?\"\n",
    "print(f\"Question: {question1}\")\n",
    "response1 = rag_system.chat(question1, thread_id=\"demo_session\")\n",
    "print(f\"Response: {response1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Ask about healthcare reform\n",
    "question2 = \"How does HR1968 affect funding for low-income programs?\"\n",
    "print(f\"Question: {question2}\")\n",
    "response2 = rag_system.chat(question2, thread_id=\"demo_session\")\n",
    "print(f\"Response: {response2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Ask a follow-up question to test memory\n",
    "question3 = \"When will these funding mechanisms be implemented?\"\n",
    "print(f\"Question: {question3}\")\n",
    "response3 = rag_system.chat(question3, thread_id=\"demo_session\")\n",
    "print(f\"Response: {response3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Ask about environmental regulations\n",
    "question4 = \"What are the new carbon emission targets for power plants?\"\n",
    "print(f\"Question: {question4}\")\n",
    "response4 = rag_system.chat(question4, thread_id=\"demo_session\")\n",
    "print(f\"Response: {response4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5: Ask about something not in the documents\n",
    "question5 = \"What is the government's policy on international trade?\"\n",
    "print(f\"Question: {question5}\")\n",
    "response5 = rag_system.chat(question5, thread_id=\"demo_session\")\n",
    "print(f\"Response: {response5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Get Conversation History\n",
    "\n",
    "Let's retrieve the full conversation history to verify that our memory is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get conversation history\n",
    "history = rag_system.get_conversation_history(thread_id=\"demo_session\")\n",
    "for i, message in enumerate(history):\n",
    "    if message.role == \"user\":\n",
    "        print(f\"User: {message.content}\")\n",
    "    else:\n",
    "        print(f\"Assistant: {message.content[:100]}...\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Working with Your Own Government Documents\n",
    "\n",
    "To use this system with your own government documents:\n",
    "\n",
    "1. Create a directory for your documents\n",
    "2. Add your PDF and/or text files to this directory\n",
    "3. Update the document processor to point to your directory\n",
    "4. Run the processing and vector store creation steps\n",
    "5. Initialize the RAG system with your retriever\n",
    "\n",
    "Here's a template for how you would do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Set up with your own documents\n",
    "'''\n",
    "# Define your documents directory\n",
    "my_docs_dir = \"./my_government_docs\"\n",
    "\n",
    "# Process your documents\n",
    "doc_processor = DocumentProcessor(docs_dir=my_docs_dir)\n",
    "raw_docs = doc_processor.load_documents()\n",
    "processed_docs = doc_processor.process_documents(raw_docs)\n",
    "\n",
    "# Set up vector store with your documents\n",
    "vector_store = VectorStore(persist_directory=\"./my_government_docs_db\")\n",
    "vector_db = vector_store.create_or_load_db(documents=processed_docs)\n",
    "retriever = vector_store.get_retriever(k=4)\n",
    "\n",
    "# Set up RAG system\n",
    "rag_system = GovernmentDocsRAG()\n",
    "rag_system.build_graph(retriever)\n",
    "\n",
    "# Start interacting\n",
    "response = rag_system.chat(\"What is the government's policy on X?\", thread_id=\"my_session\")\n",
    "print(response)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Langgraph Python 3.11",
   "language": "python",
   "name": "langgraph-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
