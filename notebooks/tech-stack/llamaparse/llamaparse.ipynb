{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cloud_services import LlamaParse\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "def set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "set_env(\"LLAMA_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id a6b63ae5-e15a-4306-9d14-a560958413f7\n"
     ]
    }
   ],
   "source": [
    "parser = LlamaParse(\n",
    "    api_key = os.environ.get(\"LLAMA_API_KEY\"),\n",
    "    num_workers=4,\n",
    "    verbose=True,\n",
    "    language=\"en\",\n",
    ")\n",
    "\n",
    "source = \"documents/H.R.1968 - Full-Year Continuing Appropriations and Extensions Act, 03-14-2025.pdf\"\n",
    "\n",
    "result = parser.parse(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LlamaParse in module llama_cloud_services.parse.base:\n",
      "\n",
      "class LlamaParse(llama_index.core.readers.base.BasePydanticReader)\n",
      " |  LlamaParse(*, is_remote: bool = False, api_key: str = '', base_url: str = 'https://api.cloud.llamaindex.ai', organization_id: Optional[str] = None, project_id: Optional[str] = None, check_interval: int = 1, backoff_pattern: llama_cloud_services.parse.base.BackoffPattern = <BackoffPattern.LINEAR: 'linear'>, max_check_interval: int = 5, custom_client: Optional[httpx.AsyncClient] = None, ignore_errors: bool = True, max_timeout: int = 2000, num_workers: typing.Annotated[int, Gt(gt=0), Lt(lt=20)] = 4, result_type: llama_cloud_services.parse.utils.ResultType = <ResultType.TXT: 'text'>, show_progress: bool = True, split_by_page: bool = True, verbose: bool = True, adaptive_long_table: Optional[bool] = False, annotate_links: Optional[bool] = False, auto_mode: Optional[bool] = False, auto_mode_configuration_json: Optional[str] = None, auto_mode_trigger_on_image_in_page: Optional[bool] = False, auto_mode_trigger_on_table_in_page: Optional[bool] = False, auto_mode_trigger_on_text_in_page: Optional[str] = None, auto_mode_trigger_on_regexp_in_page: Optional[str] = None, azure_openai_api_version: Optional[str] = None, azure_openai_deployment_name: Optional[str] = None, azure_openai_endpoint: Optional[str] = None, azure_openai_key: Optional[str] = None, bbox_bottom: Optional[float] = None, bbox_left: Optional[float] = None, bbox_right: Optional[float] = None, bbox_top: Optional[float] = None, compact_markdown_table: Optional[bool] = False, continuous_mode: Optional[bool] = False, disable_ocr: Optional[bool] = False, disable_image_extraction: Optional[bool] = False, do_not_cache: Optional[bool] = False, do_not_unroll_columns: Optional[bool] = False, extract_charts: Optional[bool] = False, extract_layout: Optional[bool] = False, fast_mode: Optional[bool] = False, guess_xlsx_sheet_names: Optional[bool] = False, html_make_all_elements_visible: Optional[bool] = False, html_remove_fixed_elements: Optional[bool] = False, html_remove_navigation_elements: Optional[bool] = False, http_proxy: Optional[str] = None, ignore_document_elements_for_layout_detection: Optional[bool] = False, input_s3_region: Optional[str] = None, invalidate_cache: Optional[bool] = False, job_timeout_extra_time_per_page_in_seconds: Optional[float] = None, job_timeout_in_seconds: Optional[float] = None, language: Optional[str] = 'en', markdown_table_multiline_header_separator: Optional[str] = None, max_pages: Optional[int] = None, output_pdf_of_document: Optional[bool] = False, output_s3_path_prefix: Optional[str] = None, output_s3_region: Optional[str] = None, output_tables_as_HTML: Optional[bool] = False, outlined_table_extraction: Optional[bool] = False, page_error_tolerance: Optional[float] = None, page_prefix: Optional[str] = None, page_separator: Optional[str] = None, page_suffix: Optional[str] = None, parse_mode: Union[llama_cloud_services.parse.utils.ParsingMode, str, NoneType] = None, premium_mode: Optional[bool] = False, preset: Optional[str] = None, preserve_layout_alignment_across_pages: Optional[bool] = False, replace_failed_page_mode: Optional[llama_cloud_services.parse.utils.FailedPageMode] = None, replace_failed_page_with_error_message_prefix: Optional[str] = None, replace_failed_page_with_error_message_suffix: Optional[str] = None, skip_diagonal_text: Optional[bool] = False, spreadsheet_extract_sub_tables: Optional[bool] = False, strict_mode_buggy_font: Optional[bool] = False, strict_mode_image_extraction: Optional[bool] = False, strict_mode_image_ocr: Optional[bool] = False, strict_mode_reconstruction: Optional[bool] = False, structured_output: Optional[bool] = False, structured_output_json_schema: Optional[str] = None, structured_output_json_schema_name: Optional[str] = None, system_prompt: Optional[str] = None, system_prompt_append: Optional[str] = None, take_screenshot: Optional[bool] = False, target_pages: Optional[str] = None, user_prompt: Optional[str] = None, vendor_multimodal_api_key: Optional[str] = None, vendor_multimodal_model_name: Optional[str] = None, model: Optional[str] = None, webhook_url: Optional[str] = None, bounding_box: Optional[str] = None, complemental_formatting_instruction: Optional[str] = None, content_guideline_instruction: Optional[str] = None, formatting_instruction: Optional[str] = None, gpt4o_mode: Optional[bool] = False, gpt4o_api_key: Optional[str] = None, is_formatting_instruction: Optional[bool] = False, parsing_instruction: Optional[str] = '', use_vendor_multimodal_model: Optional[bool] = False, partition_pages: Optional[int] = None) -> None\n",
      " |  \n",
      " |  A smart-parser for files.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LlamaParse\n",
      " |      llama_index.core.readers.base.BasePydanticReader\n",
      " |      llama_index.core.readers.base.BaseReader\n",
      " |      abc.ABC\n",
      " |      llama_index.core.schema.BaseComponent\n",
      " |      pydantic.main.BaseModel\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  async aget_assets(self, json_result: List[dict], download_path: str, asset_key: str) -> List[dict]\n",
      " |      Download assets (images or charts) from the parsed result.\n",
      " |  \n",
      " |  async aget_charts(self, json_result: List[dict], download_path: str) -> List[dict]\n",
      " |      Download charts from the parsed result.\n",
      " |  \n",
      " |  async aget_images(self, json_result: List[dict], download_path: str) -> List[dict]\n",
      " |      Download images from the parsed result.\n",
      " |  \n",
      " |  async aget_json(self, file_path: Union[List[Union[str, bytes, io.BufferedIOBase]], str, bytes, io.BufferedIOBase], extra_info: Optional[dict] = None) -> List[dict]\n",
      " |      Load data from the input path.\n",
      " |  \n",
      " |  async aget_xlsx(self, json_result: List[dict], download_path: str) -> List[dict]\n",
      " |      Download xlsx from the parsed result.\n",
      " |  \n",
      " |  async aload_data(self, file_path: Union[List[Union[str, bytes, io.BufferedIOBase]], str, bytes, io.BufferedIOBase], extra_info: Optional[dict] = None, fs: Optional[fsspec.spec.AbstractFileSystem] = None) -> List[llama_index.core.schema.Document]\n",
      " |      Load data from the input path.\n",
      " |      \n",
      " |      File(s) which were partitioned before parsing will be loaded as a single\n",
      " |      re-assembled Document.\n",
      " |  \n",
      " |  async aparse(self, file_path: Union[List[Union[str, bytes, io.BufferedIOBase]], str, bytes, io.BufferedIOBase], extra_info: Optional[dict] = None, fs: Optional[fsspec.spec.AbstractFileSystem] = None) -> Union[List[ForwardRef('JobResult')], ForwardRef('JobResult')]\n",
      " |      Parse the file and return a JobResult object instead of Document objects.\n",
      " |      \n",
      " |      This method is similar to aload_data but returns JobResult objects that provide\n",
      " |      direct access to the various output formats (text, markdown, json, etc.)\n",
      " |      \n",
      " |      Args:\n",
      " |          file_path: Path to the file to parse. Can be a string, path, bytes, file-like object, or a list of these.\n",
      " |          extra_info: Additional metadata to include in the result.\n",
      " |          fs: Optional filesystem to use for reading files.\n",
      " |      \n",
      " |      Returns:\n",
      " |          JobResult object or list of JobResult objects if either multiple files were provided or file(s) were partitioned before parsing.\n",
      " |  \n",
      " |  client_context(self) -> AsyncGenerator[httpx.AsyncClient, NoneType]\n",
      " |      Create a context for the HTTPX client.\n",
      " |  \n",
      " |  get_charts(self, json_result: List[dict], download_path: str) -> List[dict]\n",
      " |      Download charts from the parsed result.\n",
      " |  \n",
      " |  get_images(self, json_result: List[dict], download_path: str) -> List[dict]\n",
      " |      Download images from the parsed result.\n",
      " |  \n",
      " |  get_json(self, file_path: Union[List[Union[str, bytes, io.BufferedIOBase]], str, bytes, io.BufferedIOBase], extra_info: Optional[dict] = None) -> List[dict]\n",
      " |      Load data from the input path.\n",
      " |  \n",
      " |  get_json_result(self, file_path: Union[List[Union[str, bytes, io.BufferedIOBase]], str, bytes, io.BufferedIOBase], extra_info: Optional[dict] = None) -> List[dict]\n",
      " |      Parse the input path.\n",
      " |  \n",
      " |  get_xlsx(self, json_result: List[dict], download_path: str) -> List[dict]\n",
      " |      Download xlsx from the parsed result.\n",
      " |  \n",
      " |  load_data(self, file_path: Union[List[Union[str, bytes, io.BufferedIOBase]], str, bytes, io.BufferedIOBase], extra_info: Optional[dict] = None, fs: Optional[fsspec.spec.AbstractFileSystem] = None) -> List[llama_index.core.schema.Document]\n",
      " |      Load data from the input path.\n",
      " |  \n",
      " |  model_post_init = init_private_attributes(self: 'BaseModel', context: 'Any', /) -> 'None' from pydantic._internal._model_construction\n",
      " |      This function is meant to behave like a BaseModel method to initialise private attributes.\n",
      " |      \n",
      " |      It takes context as an argument since that's what pydantic-core passes when calling it.\n",
      " |      \n",
      " |      Args:\n",
      " |          self: The BaseModel instance.\n",
      " |          context: The context.\n",
      " |  \n",
      " |  parse(self, file_path: Union[List[Union[str, bytes, io.BufferedIOBase]], str, bytes, io.BufferedIOBase], extra_info: Optional[dict] = None, fs: Optional[fsspec.spec.AbstractFileSystem] = None) -> Union[List[ForwardRef('JobResult')], ForwardRef('JobResult')]\n",
      " |      Parse the file and return a JobResult object instead of Document objects.\n",
      " |      \n",
      " |      This method is similar to load_data but returns JobResult objects that provide\n",
      " |      direct access to the various output formats (text, markdown, json, etc.)\n",
      " |      \n",
      " |      Args:\n",
      " |          file_path: Path to the file to parse. Can be a string, path, bytes, file-like object, or a list of these.\n",
      " |          extra_info: Additional metadata to include in the result.\n",
      " |          fs: Optional filesystem to use for reading files.\n",
      " |      \n",
      " |      Returns:\n",
      " |          JobResult object or list of JobResult objects if multiple files were provided\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  validate_api_key(v: str) -> str\n",
      " |      Validate the API key.\n",
      " |  \n",
      " |  validate_base_url(v: str) -> str\n",
      " |      Validate the base URL.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  aclient\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'_aclient': typing.Optional[httpx.AsyncClient], 'ad...\n",
      " |  \n",
      " |  __class_vars__ = set()\n",
      " |  \n",
      " |  __private_attributes__ = {'_aclient': ModelPrivateAttr()}\n",
      " |  \n",
      " |  __pydantic_complete__ = True\n",
      " |  \n",
      " |  __pydantic_computed_fields__ = {}\n",
      " |  \n",
      " |  __pydantic_core_schema__ = {'cls': <class 'llama_cloud_services.parse....\n",
      " |  \n",
      " |  __pydantic_custom_init__ = False\n",
      " |  \n",
      " |  __pydantic_decorators__ = DecoratorInfos(validators={}, field_validato...\n",
      " |  \n",
      " |  __pydantic_fields__ = {'adaptive_long_table': FieldInfo(annotation=Uni...\n",
      " |  \n",
      " |  __pydantic_generic_metadata__ = {'args': (), 'origin': None, 'paramete...\n",
      " |  \n",
      " |  __pydantic_parent_namespace__ = None\n",
      " |  \n",
      " |  __pydantic_post_init__ = 'model_post_init'\n",
      " |  \n",
      " |  __pydantic_serializer__ = SchemaSerializer(serializer=FunctionWrap(\n",
      " |    ...\n",
      " |  \n",
      " |  __pydantic_setattr_handlers__ = {'_aclient': <function _private_setatt...\n",
      " |  \n",
      " |  __pydantic_validator__ = SchemaValidator(title=\"LlamaParse\", validator...\n",
      " |  \n",
      " |  __signature__ = <Signature (*, is_remote: bool = False, api_key:..., p...\n",
      " |  \n",
      " |  model_config = {'arbitrary_types_allowed': True}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from llama_index.core.readers.base.BaseReader:\n",
      " |  \n",
      " |  async alazy_load_data(self, *args: Any, **load_kwargs: Any) -> Iterable[llama_index.core.schema.Document]\n",
      " |      Load data from the input directory lazily.\n",
      " |  \n",
      " |  lazy_load_data(self, *args: Any, **load_kwargs: Any) -> Iterable[llama_index.core.schema.Document]\n",
      " |      Load data from the input directory lazily.\n",
      " |  \n",
      " |  load_langchain_documents(self, **load_kwargs: Any) -> List[ForwardRef('LCDocument')]\n",
      " |      Load data in LangChain document format.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from llama_index.core.readers.base.BaseReader:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from llama_index.core.schema.BaseComponent:\n",
      " |  \n",
      " |  __getstate__(self) -> 'Dict[str, Any]'\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  __setstate__(self, state: 'Dict[str, Any]') -> 'None'\n",
      " |  \n",
      " |  custom_model_dump(self, handler: 'SerializerFunctionWrapHandler', info: 'SerializationInfo') -> 'Dict[str, Any]'\n",
      " |  \n",
      " |  dict(self, **kwargs: 'Any') -> 'Dict[str, Any]'\n",
      " |  \n",
      " |  json(self, **kwargs: 'Any') -> 'str'\n",
      " |  \n",
      " |  to_dict(self, **kwargs: 'Any') -> 'Dict[str, Any]'\n",
      " |  \n",
      " |  to_json(self, **kwargs: 'Any') -> 'str'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from llama_index.core.schema.BaseComponent:\n",
      " |  \n",
      " |  __get_pydantic_json_schema__(core_schema: 'CoreSchema', handler: 'GetJsonSchemaHandler') -> 'JsonSchemaValue'\n",
      " |      Hook into generating the model's JSON schema.\n",
      " |      \n",
      " |      Args:\n",
      " |          core_schema: A `pydantic-core` CoreSchema.\n",
      " |              You can ignore this argument and call the handler with a new CoreSchema,\n",
      " |              wrap this CoreSchema (`{'type': 'nullable', 'schema': current_schema}`),\n",
      " |              or just call the handler with the original schema.\n",
      " |          handler: Call into Pydantic's internal JSON schema generation.\n",
      " |              This will raise a `pydantic.errors.PydanticInvalidForJsonSchema` if JSON schema\n",
      " |              generation fails.\n",
      " |              Since this gets called by `BaseModel.model_json_schema` you can override the\n",
      " |              `schema_generator` argument to that function to change JSON schema generation globally\n",
      " |              for a type.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A JSON schema, as a Python object.\n",
      " |  \n",
      " |  class_name() -> 'str'\n",
      " |      Get the class name, used as a unique ID in serialization.\n",
      " |      \n",
      " |      This provides a key that makes serialization robust against actual class\n",
      " |      name changes.\n",
      " |  \n",
      " |  from_dict(data: 'Dict[str, Any]', **kwargs: 'Any') -> 'Self'\n",
      " |      # TODO: return type here not supported by current mypy version\n",
      " |  \n",
      " |  from_json(data_str: 'str', **kwargs: 'Any') -> 'Self'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __copy__(self) -> 'Self'\n",
      " |      Returns a shallow copy of the model.\n",
      " |  \n",
      " |  __deepcopy__(self, memo: 'dict[int, Any] | None' = None) -> 'Self'\n",
      " |      Returns a deep copy of the model.\n",
      " |  \n",
      " |  __delattr__(self, item: 'str') -> 'Any'\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __eq__(self, other: 'Any') -> 'bool'\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __getattr__(self, item: 'str') -> 'Any'\n",
      " |  \n",
      " |  __init__(self, /, **data: 'Any') -> 'None'\n",
      " |      Create a new model by parsing and validating input data from keyword arguments.\n",
      " |      \n",
      " |      Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n",
      " |      validated to form a valid model.\n",
      " |      \n",
      " |      `self` is explicitly positional-only to allow `self` as a field name.\n",
      " |  \n",
      " |  __iter__(self) -> 'TupleGenerator'\n",
      " |      So `dict(model)` works.\n",
      " |  \n",
      " |  __pretty__(self, fmt: 'typing.Callable[[Any], Any]', **kwargs: 'Any') -> 'typing.Generator[Any, None, None]' from pydantic._internal._repr.Representation\n",
      " |      Used by devtools (https://python-devtools.helpmanual.io/) to pretty print objects.\n",
      " |  \n",
      " |  __replace__(self, **changes: 'Any') -> 'Self'\n",
      " |      # Because we make use of `@dataclass_transform()`, `__replace__` is already synthesized by\n",
      " |      # type checkers, so we define the implementation in this `if not TYPE_CHECKING:` block:\n",
      " |  \n",
      " |  __repr__(self) -> 'str'\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __repr_args__(self) -> '_repr.ReprArgs'\n",
      " |  \n",
      " |  __repr_name__(self) -> 'str' from pydantic._internal._repr.Representation\n",
      " |      Name of the instance's class, used in __repr__.\n",
      " |  \n",
      " |  __repr_recursion__(self, object: 'Any') -> 'str' from pydantic._internal._repr.Representation\n",
      " |      Returns the string representation of a recursive object.\n",
      " |  \n",
      " |  __repr_str__(self, join_str: 'str') -> 'str' from pydantic._internal._repr.Representation\n",
      " |  \n",
      " |  __rich_repr__(self) -> 'RichReprResult' from pydantic._internal._repr.Representation\n",
      " |      Used by Rich (https://rich.readthedocs.io/en/stable/pretty.html) to pretty print objects.\n",
      " |  \n",
      " |  __setattr__(self, name: 'str', value: 'Any') -> 'None'\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __str__(self) -> 'str'\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  copy(self, *, include: 'AbstractSetIntStr | MappingIntStrAny | None' = None, exclude: 'AbstractSetIntStr | MappingIntStrAny | None' = None, update: 'Dict[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      " |      Returns a copy of the model.\n",
      " |      \n",
      " |      !!! warning \"Deprecated\"\n",
      " |          This method is now deprecated; use `model_copy` instead.\n",
      " |      \n",
      " |      If you need `include` or `exclude`, use:\n",
      " |      \n",
      " |      ```python {test=\"skip\" lint=\"skip\"}\n",
      " |      data = self.model_dump(include=include, exclude=exclude, round_trip=True)\n",
      " |      data = {**data, **(update or {})}\n",
      " |      copied = self.model_validate(data)\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |          include: Optional set or mapping specifying which fields to include in the copied model.\n",
      " |          exclude: Optional set or mapping specifying which fields to exclude in the copied model.\n",
      " |          update: Optional dictionary of field-value pairs to override field values in the copied model.\n",
      " |          deep: If True, the values of fields that are Pydantic models will be deep-copied.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A copy of the model with included, excluded and updated fields as specified.\n",
      " |  \n",
      " |  model_copy(self, *, update: 'Mapping[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      " |      !!! abstract \"Usage Documentation\"\n",
      " |          [`model_copy`](../concepts/serialization.md#model_copy)\n",
      " |      \n",
      " |      Returns a copy of the model.\n",
      " |      \n",
      " |      !!! note\n",
      " |          The underlying instance's [`__dict__`][object.__dict__] attribute is copied. This\n",
      " |          might have unexpected side effects if you store anything in it, on top of the model\n",
      " |          fields (e.g. the value of [cached properties][functools.cached_property]).\n",
      " |      \n",
      " |      Args:\n",
      " |          update: Values to change/add in the new model. Note: the data is not validated\n",
      " |              before creating the new model. You should trust this data.\n",
      " |          deep: Set to `True` to make a deep copy of the model.\n",
      " |      \n",
      " |      Returns:\n",
      " |          New model instance.\n",
      " |  \n",
      " |  model_dump(self, *, mode: \"Literal['json', 'python'] | str\" = 'python', include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool | None' = None, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, fallback: 'Callable[[Any], Any] | None' = None, serialize_as_any: 'bool' = False) -> 'dict[str, Any]'\n",
      " |      !!! abstract \"Usage Documentation\"\n",
      " |          [`model_dump`](../concepts/serialization.md#modelmodel_dump)\n",
      " |      \n",
      " |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode: The mode in which `to_python` should run.\n",
      " |              If mode is 'json', the output will only contain JSON serializable types.\n",
      " |              If mode is 'python', the output may contain non-JSON-serializable Python objects.\n",
      " |          include: A set of fields to include in the output.\n",
      " |          exclude: A set of fields to exclude from the output.\n",
      " |          context: Additional context to pass to the serializer.\n",
      " |          by_alias: Whether to use the field's alias in the dictionary key if defined.\n",
      " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      " |          fallback: A function to call when an unknown value is encountered. If not provided,\n",
      " |              a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError] error is raised.\n",
      " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dictionary representation of the model.\n",
      " |  \n",
      " |  model_dump_json(self, *, indent: 'int | None' = None, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool | None' = None, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, fallback: 'Callable[[Any], Any] | None' = None, serialize_as_any: 'bool' = False) -> 'str'\n",
      " |      !!! abstract \"Usage Documentation\"\n",
      " |          [`model_dump_json`](../concepts/serialization.md#modelmodel_dump_json)\n",
      " |      \n",
      " |      Generates a JSON representation of the model using Pydantic's `to_json` method.\n",
      " |      \n",
      " |      Args:\n",
      " |          indent: Indentation to use in the JSON output. If None is passed, the output will be compact.\n",
      " |          include: Field(s) to include in the JSON output.\n",
      " |          exclude: Field(s) to exclude from the JSON output.\n",
      " |          context: Additional context to pass to the serializer.\n",
      " |          by_alias: Whether to serialize using field aliases.\n",
      " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      " |          fallback: A function to call when an unknown value is encountered. If not provided,\n",
      " |              a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError] error is raised.\n",
      " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A JSON string representation of the model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __class_getitem__(typevar_values: 'type[Any] | tuple[type[Any], ...]') -> 'type[BaseModel] | _forward_ref.PydanticRecursiveRef'\n",
      " |  \n",
      " |  __get_pydantic_core_schema__(source: 'type[BaseModel]', handler: 'GetCoreSchemaHandler', /) -> 'CoreSchema'\n",
      " |  \n",
      " |  __pydantic_init_subclass__(**kwargs: 'Any') -> 'None'\n",
      " |      This is intended to behave just like `__init_subclass__`, but is called by `ModelMetaclass`\n",
      " |      only after the class is actually fully initialized. In particular, attributes like `model_fields` will\n",
      " |      be present when this is called.\n",
      " |      \n",
      " |      This is necessary because `__init_subclass__` will always be called by `type.__new__`,\n",
      " |      and it would require a prohibitively large refactor to the `ModelMetaclass` to ensure that\n",
      " |      `type.__new__` was called in such a manner that the class would already be sufficiently initialized.\n",
      " |      \n",
      " |      This will receive the same `kwargs` that would be passed to the standard `__init_subclass__`, namely,\n",
      " |      any kwargs passed to the class definition that aren't used internally by pydantic.\n",
      " |      \n",
      " |      Args:\n",
      " |          **kwargs: Any keyword arguments passed to the class definition that aren't used internally\n",
      " |              by pydantic.\n",
      " |  \n",
      " |  construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self'\n",
      " |  \n",
      " |  from_orm(obj: 'Any') -> 'Self'\n",
      " |  \n",
      " |  model_construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self'\n",
      " |      Creates a new instance of the `Model` class with validated data.\n",
      " |      \n",
      " |      Creates a new model setting `__dict__` and `__pydantic_fields_set__` from trusted or pre-validated data.\n",
      " |      Default values are respected, but no other validation is performed.\n",
      " |      \n",
      " |      !!! note\n",
      " |          `model_construct()` generally respects the `model_config.extra` setting on the provided model.\n",
      " |          That is, if `model_config.extra == 'allow'`, then all extra passed values are added to the model instance's `__dict__`\n",
      " |          and `__pydantic_extra__` fields. If `model_config.extra == 'ignore'` (the default), then all extra passed values are ignored.\n",
      " |          Because no validation is performed with a call to `model_construct()`, having `model_config.extra == 'forbid'` does not result in\n",
      " |          an error if extra values are passed, but they will be ignored.\n",
      " |      \n",
      " |      Args:\n",
      " |          _fields_set: A set of field names that were originally explicitly set during instantiation. If provided,\n",
      " |              this is directly used for the [`model_fields_set`][pydantic.BaseModel.model_fields_set] attribute.\n",
      " |              Otherwise, the field names from the `values` argument will be used.\n",
      " |          values: Trusted or pre-validated data dictionary.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new instance of the `Model` class with validated data.\n",
      " |  \n",
      " |  model_json_schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', schema_generator: 'type[GenerateJsonSchema]' = <class 'pydantic.json_schema.GenerateJsonSchema'>, mode: 'JsonSchemaMode' = 'validation') -> 'dict[str, Any]'\n",
      " |      Generates a JSON schema for a model class.\n",
      " |      \n",
      " |      Args:\n",
      " |          by_alias: Whether to use attribute aliases or not.\n",
      " |          ref_template: The reference template.\n",
      " |          schema_generator: To override the logic used to generate the JSON schema, as a subclass of\n",
      " |              `GenerateJsonSchema` with your desired modifications\n",
      " |          mode: The mode in which to generate the schema.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The JSON schema for the given model class.\n",
      " |  \n",
      " |  model_parametrized_name(params: 'tuple[type[Any], ...]') -> 'str'\n",
      " |      Compute the class name for parametrizations of generic classes.\n",
      " |      \n",
      " |      This method can be overridden to achieve a custom naming scheme for generic BaseModels.\n",
      " |      \n",
      " |      Args:\n",
      " |          params: Tuple of types of the class. Given a generic class\n",
      " |              `Model` with 2 type variables and a concrete model `Model[str, int]`,\n",
      " |              the value `(str, int)` would be passed to `params`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          String representing the new class where `params` are passed to `cls` as type variables.\n",
      " |      \n",
      " |      Raises:\n",
      " |          TypeError: Raised when trying to generate concrete names for non-generic models.\n",
      " |  \n",
      " |  model_rebuild(*, force: 'bool' = False, raise_errors: 'bool' = True, _parent_namespace_depth: 'int' = 2, _types_namespace: 'MappingNamespace | None' = None) -> 'bool | None'\n",
      " |      Try to rebuild the pydantic-core schema for the model.\n",
      " |      \n",
      " |      This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\n",
      " |      the initial attempt to build the schema, and automatic rebuilding fails.\n",
      " |      \n",
      " |      Args:\n",
      " |          force: Whether to force the rebuilding of the model schema, defaults to `False`.\n",
      " |          raise_errors: Whether to raise errors, defaults to `True`.\n",
      " |          _parent_namespace_depth: The depth level of the parent namespace, defaults to 2.\n",
      " |          _types_namespace: The types namespace, defaults to `None`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Returns `None` if the schema is already \"complete\" and rebuilding was not required.\n",
      " |          If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.\n",
      " |  \n",
      " |  model_validate(obj: 'Any', *, strict: 'bool | None' = None, from_attributes: 'bool | None' = None, context: 'Any | None' = None, by_alias: 'bool | None' = None, by_name: 'bool | None' = None) -> 'Self'\n",
      " |      Validate a pydantic model instance.\n",
      " |      \n",
      " |      Args:\n",
      " |          obj: The object to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          from_attributes: Whether to extract data from object attributes.\n",
      " |          context: Additional context to pass to the validator.\n",
      " |          by_alias: Whether to use the field's alias when validating against the provided input data.\n",
      " |          by_name: Whether to use the field's name when validating against the provided input data.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValidationError: If the object could not be validated.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The validated model instance.\n",
      " |  \n",
      " |  model_validate_json(json_data: 'str | bytes | bytearray', *, strict: 'bool | None' = None, context: 'Any | None' = None, by_alias: 'bool | None' = None, by_name: 'bool | None' = None) -> 'Self'\n",
      " |      !!! abstract \"Usage Documentation\"\n",
      " |          [JSON Parsing](../concepts/json.md#json-parsing)\n",
      " |      \n",
      " |      Validate the given JSON data against the Pydantic model.\n",
      " |      \n",
      " |      Args:\n",
      " |          json_data: The JSON data to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          context: Extra variables to pass to the validator.\n",
      " |          by_alias: Whether to use the field's alias when validating against the provided input data.\n",
      " |          by_name: Whether to use the field's name when validating against the provided input data.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The validated Pydantic model.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValidationError: If `json_data` is not a JSON string or the object could not be validated.\n",
      " |  \n",
      " |  model_validate_strings(obj: 'Any', *, strict: 'bool | None' = None, context: 'Any | None' = None, by_alias: 'bool | None' = None, by_name: 'bool | None' = None) -> 'Self'\n",
      " |      Validate the given object with string data against the Pydantic model.\n",
      " |      \n",
      " |      Args:\n",
      " |          obj: The object containing string data to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          context: Extra variables to pass to the validator.\n",
      " |          by_alias: Whether to use the field's alias when validating against the provided input data.\n",
      " |          by_name: Whether to use the field's name when validating against the provided input data.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The validated Pydantic model.\n",
      " |  \n",
      " |  parse_file(path: 'str | Path', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self'\n",
      " |  \n",
      " |  parse_obj(obj: 'Any') -> 'Self'\n",
      " |  \n",
      " |  parse_raw(b: 'str | bytes', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self'\n",
      " |  \n",
      " |  schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}') -> 'Dict[str, Any]'\n",
      " |  \n",
      " |  schema_json(*, by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', **dumps_kwargs: 'Any') -> 'str'\n",
      " |  \n",
      " |  update_forward_refs(**localns: 'Any') -> 'None'\n",
      " |  \n",
      " |  validate(value: 'Any') -> 'Self'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __fields_set__\n",
      " |  \n",
      " |  model_extra\n",
      " |      Get extra fields set during validation.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dictionary of extra fields, or `None` if `config.extra` is not set to `\"allow\"`.\n",
      " |  \n",
      " |  model_fields_set\n",
      " |      Returns the set of fields that have been explicitly set on this model instance.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A set of strings representing the fields that have been set,\n",
      " |              i.e. that were not filled from defaults.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __pydantic_extra__\n",
      " |  \n",
      " |  __pydantic_fields_set__\n",
      " |  \n",
      " |  __pydantic_private__\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  __pydantic_root_model__ = False\n",
      " |  \n",
      " |  model_computed_fields = {}\n",
      " |  \n",
      " |  model_fields = {'adaptive_long_table': FieldInfo(annotation=Union[bool...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(LlamaParse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Langgraph Python 3.11",
   "language": "python",
   "name": "langgraph-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
