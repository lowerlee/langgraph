{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c602810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 25 articles.\n",
      "Error: Expected metadata value to be a str, int, float, bool, or None, got [] which is a list in add.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from chromadb import Client\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Paths\n",
    "output_path = \"/workspaces/langgraph/data/\"\n",
    "json_file = os.path.join(output_path, \"govexec_articles_latest.json\")\n",
    "vector_db_path = \"./webscraping_vectors\"\n",
    "\n",
    "# Load scraped articles\n",
    "def load_articles(filepath):\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"File not found: {filepath}\")\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Initialize embedding model\n",
    "def initialize_model():\n",
    "    return SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create or load vector database\n",
    "def initialize_vector_db(persist_directory):\n",
    "    os.makedirs(persist_directory, exist_ok=True)\n",
    "    return Client(Settings(persist_directory=persist_directory))\n",
    "\n",
    "# Process articles and store embeddings with detailed metadata\n",
    "def embed_and_store(articles, model, vector_db):\n",
    "    collection = vector_db.get_or_create_collection(\"webscraping_articles\")\n",
    "    detailed_metadata = []  # List to store detailed metadata for all articles\n",
    "\n",
    "    for i, article in enumerate(articles):\n",
    "        content = article.get(\"content\", \"\")\n",
    "        title = article.get(\"title\", \"No Title\")\n",
    "        link = article.get(\"link\", \"\")\n",
    "        date = article.get(\"date\", \"Unknown Date\")  # Assuming articles have a 'date' field\n",
    "        citations = article.get(\"citations\", [])  # Assuming articles have a 'citations' field\n",
    "        high_level_ideas = article.get(\"high_level_ideas\", [])  # Assuming articles have this field\n",
    "\n",
    "        if content:\n",
    "            embedding = model.encode(content)\n",
    "            doc_id = f\"article_{i}\"\n",
    "\n",
    "            # Add to vector database\n",
    "            collection.add(\n",
    "                ids=[doc_id],\n",
    "                documents=[content],\n",
    "                metadatas=[{\n",
    "                    \"title\": title,\n",
    "                    \"link\": link,\n",
    "                    \"date\": date,\n",
    "                    \"citations\": citations,\n",
    "                    \"high_level_ideas\": high_level_ideas\n",
    "                }],\n",
    "                embeddings=[embedding]\n",
    "            )\n",
    "\n",
    "            # Save detailed metadata\n",
    "            detailed_metadata.append({\n",
    "                \"id\": doc_id,\n",
    "                \"title\": title,\n",
    "                \"link\": link,\n",
    "                \"date\": date,\n",
    "                \"citations\": citations,\n",
    "                \"high_level_ideas\": high_level_ideas,\n",
    "                \"content\": content\n",
    "            })\n",
    "\n",
    "    # Save detailed metadata to a JSON file\n",
    "    metadata_file = os.path.join(output_path, \"detailed_metadata.json\")\n",
    "    with open(metadata_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(detailed_metadata, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Stored {len(articles)} articles in the vector database and saved detailed metadata.\")\n",
    "\n",
    "# Extract embeddings from the vector database\n",
    "def extract_embeddings(vector_db):\n",
    "    collection = vector_db.get_collection(\"webscraping_articles\")\n",
    "    results = collection.get(include=[\"embeddings\", \"metadatas\"])\n",
    "    embeddings = np.array(results[\"embeddings\"])\n",
    "    titles = [meta[\"title\"] for meta in results[\"metadatas\"]]\n",
    "    return embeddings, titles\n",
    "\n",
    "# Visualize embeddings using PCA and UMAP\n",
    "def visualize_first_article(embeddings, titles):\n",
    "    if len(embeddings) == 0:\n",
    "        print(\"No embeddings available for visualization.\")\n",
    "        return\n",
    "\n",
    "    # Select the first embedding and title\n",
    "    first_embedding = embeddings[0].reshape(1, -1)  # Reshape for PCA/UMAP compatibility\n",
    "    first_title = titles[0]\n",
    "\n",
    "    if first_embedding.shape[0] < 2:\n",
    "        print(\"Not enough data points for PCA/UMAP visualization.\")\n",
    "        return\n",
    "\n",
    "    # PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_result = pca.fit_transform(first_embedding)\n",
    "\n",
    "    # UMAP\n",
    "    umap_reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "    umap_result = umap_reducer.fit_transform(first_embedding)\n",
    "\n",
    "    # Plot PCA and UMAP\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # PCA Plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.7, s=50, label=first_title)\n",
    "    plt.title(\"PCA Visualization (First Article)\")\n",
    "    plt.xlabel(\"PCA Component 1\")\n",
    "    plt.ylabel(\"PCA Component 2\")\n",
    "    plt.legend()\n",
    "\n",
    "    # UMAP Plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(umap_result[:, 0], umap_result[:, 1], alpha=0.7, s=50, label=first_title)\n",
    "    plt.title(\"UMAP Visualization (First Article)\")\n",
    "    plt.xlabel(\"UMAP Component 1\")\n",
    "    plt.ylabel(\"UMAP Component 2\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Main workflow\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Load articles\n",
    "        articles = load_articles(json_file)\n",
    "        print(f\"Loaded {len(articles)} articles.\")\n",
    "\n",
    "        # Initialize model and vector database\n",
    "        model = initialize_model()\n",
    "        vector_db = initialize_vector_db(vector_db_path)\n",
    "\n",
    "        # Embed and store articles\n",
    "        embed_and_store(articles, model, vector_db)\n",
    "        print(\"Embeddings stored successfully!\")\n",
    "\n",
    "        # # Extract embeddings\n",
    "        # embeddings, titles = extract_embeddings(vector_db)\n",
    "        # print(f\"Extracted {len(embeddings)} embeddings.\")\n",
    "\n",
    "        # # Visualize the first article\n",
    "        # visualize_first_article(embeddings, titles)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Langgraph Python 3.11",
   "language": "python",
   "name": "langgraph-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
